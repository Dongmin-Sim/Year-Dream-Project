{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "file5_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFyZ5vetQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75eff1d-dc25-4947-e3e8-a60c119ae5cb"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "# 수학 관련 라이브러리\n",
        "import numpy as np\n",
        "import math\n",
        "# pytorch 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import tqdm\n",
        "from torch.utils import data # dataset 관련된 utility 를 사용하려는 용도\n",
        "from random import choice, randrange # random\n",
        "from itertools import zip_longest \n",
        "import json \n",
        "import random\n",
        "import pdb\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ABHpkPXsV-Gx",
        "outputId": "4bf7bf13-6032-4b5a-d46c-8ef81fa6e5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-569dae43-f2d0-4f7d-87cc-fd89410f2595\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-569dae43-f2d0-4f7d-87cc-fd89410f2595\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_files.zip to data_files.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data_files.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-eRAdsqWanC",
        "outputId": "8dabc169-5863-485c-8c5c-9a754b500aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data_files.zip\n",
            "  inflating: english_id2word.pkl     \n",
            "  inflating: english_vocab.pkl       \n",
            "  inflating: english_word2id.pkl     \n",
            "   creating: korean_data/\n",
            "  inflating: korean_data/test_english.csv  \n",
            "  inflating: korean_data/test_korean.csv  \n",
            "  inflating: korean_data/train.csv   \n",
            "  inflating: korean_id2word.pkl      \n",
            "  inflating: korean_vocab.pkl        \n",
            "  inflating: korean_word2id.pkl      \n",
            "  inflating: train_english.pkl       \n",
            "  inflating: train_korean.pkl        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYCLJa4petQg"
      },
      "source": [
        "def split_last(x, shape):\n",
        "    \"split the last dimension to given shape\"\n",
        "    shape = list(shape)\n",
        "    assert shape.count(-1) <= 1\n",
        "    if -1 in shape:\n",
        "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
        "    return x.view(*x.size()[:-1], *shape)\n",
        "\n",
        "def merge_last(x, n_dims):\n",
        "    \"merge the last n_dims to a dimension\"\n",
        "    s = x.size()\n",
        "    assert n_dims > 1 and n_dims < len(s)\n",
        "    return x.view(*s[:-n_dims], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQR9lBMetQh"
      },
      "source": [
        "# Activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295zs7CWetQj"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWS8voZZetQk"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYgHC0ietQk"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # sequence 방향 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # sequence 방향 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiPES9getQl"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbTPl6aSetQm"
      },
      "source": [
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg, vocab_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tok_embed = nn.Embedding(vocab_size, cfg.dim) # token embedding\n",
        "        self.pos_embed = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(512, cfg.dim),freeze=True) # position embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos)\n",
        "        return self.drop(self.norm(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsNAdbnetQn"
      },
      "source": [
        "#  Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ulMozSetQn"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1)) # scale\n",
        "        \n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDtvhua3etQo"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask, x_q=None):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        \n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        if x_q is None:\n",
        "            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        else:\n",
        "            q, k, v = self.proj_q(x_q), self.proj_k(x), self.proj_v(x)\n",
        "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "            scores = scores.masked_fill_(mask, -1e9)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = torch.matmul(scores, v).transpose(1,2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = merge_last(h, 2)\n",
        "        self.scores = scores\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQUnmexetQo"
      },
      "source": [
        "# Base feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMI83XbetQo"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgFSAOGNetQp"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dEoqWUFetQp"
      },
      "source": [
        "class Encoder_Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "    \n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "    \n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    subsequent_mask = torch.tensor(subsequent_mask, device=seq.device).byte()\n",
        "    return subsequent_mask\n",
        "    \n",
        "    \n",
        "class Decoder_Block(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(cfg)\n",
        "        self.encoder_attention = MultiHeadAttention(cfg)\n",
        "        \n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.proj1 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.proj2 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        \n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm3 = LayerNorm(cfg)\n",
        "        \n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "        \n",
        "    def forward(self,x , enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \n",
        "        # self-attention -> add&norm\n",
        "        h = self.self_attention(x, dec_self_attn_mask)\n",
        "        h = self.norm1(x + self.drop(self.proj1(h)))\n",
        "        \n",
        "        # encoder attention -> add&norm\n",
        "        h2 = self.self_attention(enc_outputs, dec_enc_attn_mask, x_q=h)\n",
        "        h = self.norm2(h + self.drop(self.proj2(h2))) \n",
        "        \n",
        "        # feedforward network\n",
        "        h = self.norm3(h + self.drop(self.pwff(h)))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        #====================encoder===========================\n",
        "        self.encoder_embed = Embeddings(cfg, len(korean_vocab))\n",
        "        self.encoder_blocks = nn.ModuleList([Encoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        #====================decoder============================\n",
        "        self.decoder_embed = Embeddings(cfg, len(english_vocab))\n",
        "        self.decoder_blocks = nn.ModuleList([Decoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "        \n",
        "        #=========================================================\n",
        "        self.projection = nn.Linear(cfg.dim, len(english_vocab))\n",
        "        \n",
        "        \n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        #============encoder============\n",
        "        h = self.encoder_embed(enc_inputs)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        for block in self.encoder_blocks:\n",
        "            h = block(h, enc_self_attn_mask)\n",
        "            \n",
        "        enc_outputs = h\n",
        "        \n",
        "        \n",
        "        #============decoder============\n",
        "        \n",
        "        # self attention mask\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).float()\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs).float()\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "\n",
        "        # encoder attention mask\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "        \n",
        "        \n",
        "        # embedding\n",
        "        h = self.decoder_embed(dec_inputs)\n",
        "        \n",
        "        \n",
        "        for block in self.decoder_blocks:\n",
        "            h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "        #============projection==========\n",
        "        \n",
        "        out = self.projection(h)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def greedy_decoding(self, enc_inputs, start_token_index = 1, end_token_index = 2, generation_max_len=128):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            batch_size, max_length = enc_inputs.size()\n",
        "            generation_end_flag = [0 for i in range(batch_size)]\n",
        "            predicted_sentences = []\n",
        "            #=================encoding=============\n",
        "            h = self.encoder_embed(enc_inputs)\n",
        "            enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "            for block in self.encoder_blocks:\n",
        "                h = block(h, enc_self_attn_mask)    \n",
        "            enc_outputs = h\n",
        "\n",
        "            #================ greedy decoding ==================\n",
        "            # dec_inputs : (batch size, 1) \n",
        "            dec_inputs = torch.ones(batch_size, 1, device=enc_inputs.device) * start_token_index\n",
        "            dec_inputs = dec_inputs.long()\n",
        "\n",
        "            for i in range(generation_max_len):\n",
        "\n",
        "                #====================== decoder =======================\n",
        "                # self attention mask\n",
        "                dec_self_attn_pad_mask = None\n",
        "                dec_self_attn_subsequent_mask = None\n",
        "                dec_self_attn_mask = None\n",
        "\n",
        "                # encoder attention mask\n",
        "                dec_enc_attn_mask = None\n",
        "\n",
        "\n",
        "                # embedding\n",
        "                h = self.decoder_embed(dec_inputs)\n",
        "\n",
        "\n",
        "                for block in self.decoder_blocks:\n",
        "                    h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "\n",
        "\n",
        "                out = self.projection(h[:,-1,:])\n",
        "                pred = out.argmax(-1) \n",
        "\n",
        "#                 print(out.size(), pred)\n",
        "\n",
        "                dec_inputs = torch.cat((dec_inputs, pred.unsqueeze(1)),dim=1)\n",
        "    #             print(dec_inputs)\n",
        "\n",
        "                predicted_sentences.append(pred)\n",
        "                for j, boolean in enumerate(pred==end_token_index):\n",
        "                    if boolean == True:\n",
        "                        generation_end_flag[j] = 1\n",
        "                if sum(generation_end_flag) == batch_size:\n",
        "                    break\n",
        "\n",
        "        return torch.stack(predicted_sentences, dim=1)\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5IR42wgJkFvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmYe4SsTetQq"
      },
      "source": [
        "# 한 -> 영\n",
        "data_ = pd.read_csv(\"./korean_data/train.csv\")\n",
        "korean_data = data_[\"Korean\"].values\n",
        "english_data = data_[\"English\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRZJnw7CetQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cb5ab5-9095-4617-ce95-f1d7a97d6440"
      },
      "source": [
        "nltk.tokenize.word_tokenize(korean_data[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['그것을', '막기', '위해', ',', '많은', '과학자는', '친환경적인', '사용', '방법을', '연구했어요', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weR-02iWetQq"
      },
      "source": [
        "def build_dict(seqs):\n",
        "    num_skip_sent = 0\n",
        "    word_count = 4\n",
        "    vocab = [\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"]\n",
        "    word2id = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
        "    id2word = {0: \"<pad>\", 1: \"<s>\", 2: \"</s>\", 3: \"<unk>\"}\n",
        "    print(\"Building vocab and dict..\")\n",
        "    for line in seqs:\n",
        "        words = line.strip().split(' ') # tokenized by space \n",
        "        for word in words:\n",
        "            if word not in vocab:\n",
        "                word_count += 1 # increment word_count\n",
        "                vocab.append(word) # append new unique word\n",
        "                index = word_count - 1 # word index (consider index 0)\n",
        "                word2id[word] = index # word to index\n",
        "                id2word[index] = word # index to word\n",
        "    \n",
        "    print(\"Number of unique words: %d\" % len(vocab))\n",
        "    print(\"Finised building vocab and dict!\")\n",
        "\n",
        "    return vocab, word2id, id2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "et5ZhXWOYS82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuBX1j41etQr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "6ed7e607-f936-4345-9212-e5a8cacb7943"
      },
      "source": [
        "if os.path.isfile(\"./korean_vocab.pkl\"):\n",
        "    with open(\"./train_korean.pkl\", \"rb\") as f:\n",
        "        korean_data_token_ = pickle.load(f)\n",
        "    with open(\"./train_english.pkl\", \"rb\") as f:\n",
        "        english_data_token_ = pickle.load(f)    \n",
        "    \n",
        "    with open(\"./korean_vocab.pkl\", \"rb\") as f:\n",
        "        korean_vocab = pickle.load(f)\n",
        "    with open(\"./korean_word2id.pkl\", \"rb\") as f:\n",
        "        korean_word2id = pickle.load(f)\n",
        "    with open(\"./korean_id2word.pkl\", \"rb\") as f:\n",
        "        korean_id2word = pickle.load(f)\n",
        "    with open(\"./english_vocab.pkl\", \"rb\") as f:\n",
        "        english_vocab = pickle.load(f)\n",
        "    with open(\"./english_word2id.pkl\", \"rb\") as f:\n",
        "        english_word2id = pickle.load(f)\n",
        "    with open(\"./english_id2word.pkl\", \"rb\") as f:\n",
        "        english_id2word = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    korean_data_token = []\n",
        "    for sent in korean_data:\n",
        "        korean_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "\n",
        "    english_data_token = []\n",
        "    for sent in english_data:\n",
        "        english_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "\n",
        "    korean_data_token_ = [' '.join(token) for token in korean_data_token]\n",
        "    english_data_token_ = [' '.join(token) for token in english_data_token]    \n",
        "    \n",
        "    korean_vocab, korean_word2id, korean_id2word = build_dict(korean_data_token_)\n",
        "    english_vocab, english_word2id, english_id2word = build_dict(english_data_token_)\n",
        "    \n",
        "    pickle.dump(korean_data_token_, open(\"./train_korean.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_data_token_, open(\"./train_english.pkl\", \"wb\" ))    \n",
        "    \n",
        "    pickle.dump(korean_vocab, open(\"./korean_vocab.pkl\", \"wb\" ))\n",
        "    pickle.dump(korean_word2id, open(\"./korean_word2id.pkl\", \"wb\" ))\n",
        "    pickle.dump(korean_id2word, open(\"./korean_id2word.pkl\", \"wb\" ))\n",
        "\n",
        "    pickle.dump(english_vocab, open(\"./english_vocab.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_word2id, open(\"./english_word2id.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_id2word, open(\"./english_id2word.pkl\", \"wb\" ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocab and dict..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b5ea59fae7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0menglish_data_token_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_data_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mkorean_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkorean_word2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkorean_id2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkorean_data_token_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0menglish_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_word2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_id2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_data_token_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f7320ab044cc>\u001b[0m in \u001b[0;36mbuild_dict\u001b[0;34m(seqs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tokenized by space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mword_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# increment word_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# append new unique word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efIP8QaNetQr"
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(*args)\n",
        "\n",
        "\n",
        "def pad_tensor(vec, pad, value=0, dim=0):\n",
        "    \"\"\"\n",
        "    pad token으로 채우는 용도 \n",
        "    args:\n",
        "        vec - tensor to pad\n",
        "        pad - the size to pad to\n",
        "        dim - dimension to pad\n",
        "    return:\n",
        "        a new tensor padded to 'pad' in dimension 'dim'\n",
        "    \"\"\"\n",
        "    pad_size = pad - vec.shape[0]\n",
        "\n",
        "    if len(vec.shape) == 2:\n",
        "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
        "    elif len(vec.shape) == 1:\n",
        "        zeros = torch.ones((pad_size,)) * value\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKU7nf9etQr"
      },
      "source": [
        "def pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n",
        "    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
        "    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch]\n",
        "\n",
        "    # stack all\n",
        "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
        "    ys = torch.stack([x[1] for x in batch], dim=0)\n",
        "    xs = xs[xids].contiguous() # decreasing order로 다시 나열 \n",
        "    ys = ys[xids].contiguous() # xids 와 같은 순서로 \n",
        "    target_lengths = target_lengths[xids] \n",
        "    return xs.long(), ys.long(), sequence_lengths.int(), target_lengths.int()\n",
        "\n",
        "\n",
        "class ToyDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    https://talbaumel.github.io/blog/attention/\n",
        "    \"\"\"\n",
        "    def __init__(self,  ko_path, en_path , ko_word2id, en_word2id):\n",
        "        with open(ko_path, \"rb\") as f:\n",
        "            self.ko_seqs = pickle.load(f)\n",
        "        with open(en_path, \"rb\") as f:\n",
        "            self.en_seqs = pickle.load(f)\n",
        "        self.ko_word2id = ko_word2id\n",
        "        self.en_word2id = en_word2id\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ko_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ko_seqs = self.ko_seqs[index]\n",
        "        en_seqs = self.en_seqs[index]\n",
        "        ko_seqs = self.process(ko_seqs, self.ko_word2id)\n",
        "        en_seqs = self.process(en_seqs, self.en_word2id)\n",
        "        \n",
        "        '''\n",
        "        한국어 문장 -> 영어문장\n",
        "        <s> 한국어 문장 인덱스들 </s>\n",
        "        <s> 영어 문장 인덱스들 </s>\n",
        "        '''\n",
        "        \n",
        "        return ko_seqs, en_seqs       \n",
        "\n",
        "    def process(self, seq, word2id):\n",
        "        '''\n",
        "        <s> 한국어 문장 인덱스들 </s>\n",
        "        <s> 영어 문장 인덱스들 </s>\n",
        "        '''\n",
        "\n",
        "        sequence = []\n",
        "        sequence.append(word2id[\"<s>\"])\n",
        "        words = seq.strip().split(' ')\n",
        "        for word in words:\n",
        "            if len(sequence) < model_config.max_len:\n",
        "                if word in word2id:\n",
        "                    sequence.append(word2id[word]) # \n",
        "                else:\n",
        "                    sequence.append(3) # replace by <unk> token\n",
        "            else:\n",
        "                break\n",
        "        sequence.append(word2id[\"</s>\"])\n",
        "        sequence = torch.Tensor(sequence)\n",
        "        return sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN1gCKHpetQs"
      },
      "source": [
        "sample_config = {\n",
        "    \"dim\": 32,\n",
        "    \"dim_ff\": 32,\n",
        "    \"n_layers\": 2,\n",
        "    \"p_drop_attn\": 0.1,\n",
        "    \"n_heads\": 4,\n",
        "    \"p_drop_hidden\": 0.1,\n",
        "    \"max_len\": 30,\n",
        "    \"n_segments\": 2,\n",
        "    \"vocab_size\": 30522,\n",
        "    \"batch_size\": 32\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "\n",
        "model_config = AttributeDict(sample_config)\n",
        "model = Transformer(model_config)\n",
        "model = model.cuda()\n",
        "# out = model(sample[0].cuda(),sample[0].cuda())\n",
        "# out.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XST7y_CDetQs"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Io-Gd9etQs"
      },
      "source": [
        "dataset = ToyDataset(\"train_korean.pkl\", \"train_english.pkl\", korean_word2id, english_word2id)\n",
        "train_loader = data.DataLoader(dataset, batch_size=model_config.batch_size, shuffle=True, collate_fn=pad_collate, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w17hIP50etQs"
      },
      "source": [
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    loss_list =[]\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        x, y, x_len, y_len = batch\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        logits = model(x, y[:,:-1])\n",
        "        loss = loss_fn(logits.view(-1,len(english_vocab)) , y[:,1:].contiguous().view(-1)) # loss 구하기 우리는 cross entropy 사용 \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.item())\n",
        "        \n",
        "        if (idx+1) % 400 == 0 :\n",
        "            print('epoch {} iteration {}/{} loss {:.4f}'.format(epoch+1, idx+1, len(train_loader), np.mean(loss_list)))\n",
        "            loss_list=[]\n",
        "            \n",
        "        if (idx+1) % 2000 == 0:\n",
        "            with torch.no_grad():\n",
        "                greedy_pred = model.greedy_decoding(x[0].unsqueeze(0))\n",
        "            greedy_pred_indices = greedy_pred[0].data.cpu().tolist()\n",
        "            pred_indices = logits[0].argmax(-1).data.cpu().tolist()\n",
        "            input_indices = x[0].data.cpu().tolist()\n",
        "            label_indices = y[0].data.cpu().tolist()\n",
        "            \n",
        "            if 2 in pred_indices:\n",
        "                greedy_pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                greedy_pred_len = 128\n",
        "            \n",
        "            if 2 in pred_indices:\n",
        "                pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                pred_len = 128\n",
        "\n",
        "            if 2 in input_indices:\n",
        "                input_len = input_indices.index(2)\n",
        "            else:\n",
        "                input_len = 128\n",
        "\n",
        "            if 2 in label_indices:\n",
        "                label_len = label_indices.index(2)\n",
        "            else:\n",
        "                label_len = 128\n",
        "            \n",
        "            greedy_pred_words = [english_id2word[idx] for i, idx in enumerate(greedy_pred_indices) if i<= greedy_pred_len]\n",
        "            pred_words = [english_id2word[idx] for i, idx in enumerate(pred_indices) if i<=pred_len]\n",
        "            input_words = [korean_id2word[idx] for i, idx in enumerate(input_indices) if i<=input_len]\n",
        "            label_words = [english_id2word[idx] for i, idx in enumerate(label_indices) if i<=label_len]\n",
        "\n",
        "            print('=====================================')\n",
        "            print('입력:{}'.format(' '.join(input_words[1:-1])))\n",
        "            print('출력(teacher forcing):{}'.format(' '.join(pred_words[:-1])))\n",
        "            print('출력(greedy decoding):{}'.format(' '.join(greedy_pred_words[:-1])))\n",
        "            print('정답:{}'.format(' '.join(label_words[1:-1])))\n",
        "            print('=====================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ExFwHofUetQs"
      },
      "source": [
        "#결과값 출력\n",
        "model.eval()\n",
        "for idx, batch in enumerate(train_loader):\n",
        "    x, y, x_len, y_len = batch\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    pred = model.greedy_decoding(x)\n",
        "\n",
        "    for i in range(pred.shape[0]):\n",
        "        pred_indices = pred[i].data.cpu().tolist()\n",
        "        input_indices = x[i].data.cpu().tolist()\n",
        "        label_indices = y[i].data.cpu().tolist()\n",
        "\n",
        "        if 2 in pred_indices:\n",
        "            pred_len = pred_indices.index(2)\n",
        "        else:\n",
        "            pred_len = 128\n",
        "\n",
        "        if 2 in input_indices:\n",
        "            input_len = input_indices.index(2)\n",
        "        else:\n",
        "            input_len = 128\n",
        "        \n",
        "        if 2 in label_indices:\n",
        "            label_len = label_indices.index(2)\n",
        "        else:\n",
        "            label_len = 128\n",
        "        \n",
        "        pred_words = [english_id2word[idx] for i,idx in enumerate(pred_indices) if i <= pred_len]\n",
        "        input_words = [korean_id2word[idx] for i,idx in enumerate(input_indices) if i <= input_len]\n",
        "        label_words = [english_id2word[idx] for i,idx in enumerate(label_indices) if i <= label_len]\n",
        "        if pred_len>1:\n",
        "            print('===================')\n",
        "            print('입력:{}'.format(' '.join(input_words)))\n",
        "            print('출력(greedy decoding):{}'.format(' '.join(pred_words)))\n",
        "            print('답안:{}'.format(' '.join(label_words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lnti33cetQt"
      },
      "source": [
        "# model save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2E8EQ4jetQt"
      },
      "source": [
        "path = 'trained_model.pth'\n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w97VaEnaetQt"
      },
      "source": [
        "## model load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jol3wa3jetQt"
      },
      "source": [
        "path = 'trained_model.pth'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTkUDvtietQt"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbiBFitsetQu"
      },
      "source": [
        "def test_pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x.shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x.shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [pad_tensor(x, pad=src_max_len, dim=dim) for x in batch]\n",
        "\n",
        "    # stack all\n",
        "    xs = torch.stack(batch, dim=0)\n",
        "    return xs.long(), sequence_lengths.int()\n",
        "\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self,  ko_path, ko_word2id):\n",
        "        self.load_data(ko_path)\n",
        "        self.ko_word2id = ko_word2id\n",
        "        \n",
        "        \n",
        "    def load_data(self, ko_path):\n",
        "        korean_test_data_ = pd.read_csv(\"./korean_data/test_korean.csv\")\n",
        "        korean_test_data =korean_test_data_['Korean'].iloc[:2000].values\n",
        "        korean_test_data_token = []\n",
        "        for sent in korean_test_data:\n",
        "            korean_test_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "        korean_test_data_token = [' '.join(token) for token in korean_test_data_token]\n",
        "        self.ko_seqs = korean_test_data_token\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ko_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ko_seqs = self.ko_seqs[index]\n",
        "        ko_seqs = self.process(ko_seqs, self.ko_word2id)\n",
        "        return ko_seqs       \n",
        "\n",
        "    def process(self, seq, word2id):\n",
        "        sequence = []\n",
        "        sequence.append(word2id[\"<s>\"])\n",
        "        words = seq.strip().split(' ')\n",
        "        for word in words:\n",
        "            if word in word2id:\n",
        "                sequence.append(word2id[word]) # \n",
        "            else:\n",
        "                sequence.append(3) # replace by <unk> token\n",
        "        sequence.append(word2id[\"</s>\"])\n",
        "        sequence = torch.Tensor(sequence)\n",
        "        return sequence\n",
        "    \n",
        "    \n",
        "def test(model):\n",
        "    \n",
        "    with open(\"./korean_word2id.pkl\", \"rb\") as f:\n",
        "        korean_word2id = pickle.load(f)\n",
        "    \n",
        "    test_dataset = TestDataset('./test_korean.pkl', korean_word2id)\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=model_config.batch_size, shuffle=False, collate_fn=test_pad_collate, drop_last=False)\n",
        "    model.eval()\n",
        "    j = 0\n",
        "    \n",
        "    f = open('prediction_result.txt', 'w')\n",
        "    for idx, batch in enumerate(test_loader):\n",
        "        x, x_len= batch\n",
        "        x = x.cuda()\n",
        "        pred = model.greedy_decoding(x)\n",
        "\n",
        "        for i in range(pred.shape[0]):\n",
        "            j+=1\n",
        "            \n",
        "            pred_indices = pred[i].data.cpu().tolist()\n",
        "            input_indices = x[i].data.cpu().tolist()\n",
        "\n",
        "            if 2 in pred_indices:\n",
        "                pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                pred_len = 128\n",
        "\n",
        "            if 2 in input_indices:\n",
        "                input_len = input_indices.index(2)\n",
        "            else:\n",
        "                input_len = 128\n",
        "\n",
        "            pred_words = [english_id2word[idx] for i,idx in enumerate(pred_indices) if i <= pred_len]\n",
        "            input_words = [korean_id2word[idx] for i,idx in enumerate(input_indices) if i <= input_len]\n",
        "            \n",
        "            if j%50 == 0 :\n",
        "                print('========== index {} ========='.format(j))\n",
        "                print('입력:{}'.format(' '.join(input_words[1:-1])))\n",
        "                print('출력:{}'.format(' '.join(pred_words[:-1])))\n",
        "\n",
        "            f.write(' '.join(pred_words[:-1]))\n",
        "            f.write('\\n')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Em4iwSp8etQu"
      },
      "source": [
        "test(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 과제\n",
        "\n",
        "1. query, key, value 간의 관계에 대해 작성해주세요.\n",
        "- 답변 : 기계번역이라면, 다음에 올 토큰이 무엇인지 묻는 것이 query, 정답에 대한 여러 후보들이 key, 후보들의 값을 value가 됩니다.\n",
        "2. multi-head의 필요성에 대해 작성해주세요.\n",
        "- 답변 : 여러 head로 self-attention을 시켜 단어의 여러 의미를 파악할 수 있도록 도와줍니다.\n",
        "3. BERT 사전 학습의 두가지 태스크에 대해 설명해주세요.\n",
        "- 답변 : MLM(Masked Language Model)과 NSP(Next Sentence Prediction)입니다.   \n",
        "\n",
        "4. Transformer embedding과 BERT embedding의 차이점 두가지를 작성해주세요.\n",
        "- 답변 : Transformer embedding에서는 Positional Encoding을 사용하고, BERT embedding에서는 대신에 Positional Embeddings와 Segment Embedding을 사용합니다. Segment Embedding은 단어가 첫번째 문장에 속하는지 두번째 문장에 속하는지 알려주는 역할을 하고, Positional Embedding은 각 단어가 첫번째인지, 두번째인지를 의미하는 Embedding값을 의미합니다."
      ],
      "metadata": {
        "id": "NbU1nykOxpL7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EjRI_zQetQu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8DHAo9QetQu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}